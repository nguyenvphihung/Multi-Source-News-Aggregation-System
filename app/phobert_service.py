"""
PhoBERT Service ƒë·ªÉ ph√¢n lo·∫°i b√¨nh lu·∫≠n ƒë·ªôc h·∫°i
T√≠ch h·ª£p model PhoBERT ƒë√£ train s·∫µn v√†o h·ªá th·ªëng
"""
import os
import re
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import Dict, List, Optional
import logging

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PhoBERTService:
    """Service ƒë·ªÉ ph√¢n lo·∫°i b√¨nh lu·∫≠n ƒë·ªôc h·∫°i b·∫±ng PhoBERT"""
    
    def __init__(self, model_path: str = "phobert_toxic_comment_model"):
        """
        Kh·ªüi t·∫°o PhoBERT service
        
        Args:
            model_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a model PhoBERT
        """
        self.model_path = Path(model_path)
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.is_loaded = False
        
        # Label mapping theo config.json c·ªßa model
        self.id2label = {
            0: "LABEL_0",  # B√¨nh lu·∫≠n t√≠ch c·ª±c/trung t√≠nh
            1: "LABEL_1",  # B√¨nh lu·∫≠n ti√™u c·ª±c nh·∫π
            2: "LABEL_2"   # B√¨nh lu·∫≠n ƒë·ªôc h·∫°i/spam
        }
        
        self.label_descriptions = {
            0: "B√¨nh lu·∫≠n t√≠ch c·ª±c/an to√†n",
            1: "B√¨nh lu·∫≠n ti√™u c·ª±c nh·∫π", 
            2: "B√¨nh lu·∫≠n ƒë·ªôc h·∫°i/spam"
        }
        
        # T·ª± ƒë·ªông load model khi kh·ªüi t·∫°o
        self.load_model()
    
    def load_model(self) -> bool:
        """
        Load tokenizer v√† model t·ª´ th∆∞ m·ª•c ƒë√£ ch·ªâ ƒë·ªãnh
        
        Returns:
            bool: True n·∫øu load th√†nh c√¥ng, False n·∫øu c√≥ l·ªói
        """
        try:
            if not self.model_path.exists():
                logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c model: {self.model_path}")
                return False
            
            # Ki·ªÉm tra c√°c file c·∫ßn thi·∫øt
            required_files = ["config.json", "model.safetensors", "tokenizer_config.json"]
            for file in required_files:
                if not (self.model_path / file).exists():
                    logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {file}")
                    return False
            
            logger.info(f"üîÑ ƒêang load PhoBERT model t·ª´ {self.model_path}...")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path),
                local_files_only=True,
                trust_remote_code=True
            )
            
            # Load model
            self.model = AutoModelForSequenceClassification.from_pretrained(
                str(self.model_path),
                local_files_only=True,
                trust_remote_code=True
            )
            
            # Chuy·ªÉn model sang device ph√π h·ª£p
            self.model.to(self.device)
            self.model.eval()  # Ch·∫ø ƒë·ªô evaluation
            
            self.is_loaded = True
            logger.info(f"‚úÖ PhoBERT model loaded th√†nh c√¥ng tr√™n {self.device}")
            logger.info(f"üìã Model labels: {self.id2label}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load PhoBERT model: {e}")
            self.is_loaded = False
            return False
    
    def preprocess_text(self, text: str) -> str:
        """
        Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n tr∆∞·ªõc khi ƒë∆∞a v√†o model
        
        Args:
            text (str): VƒÉn b·∫£n g·ªëc
            
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        if not text:
            return ""
        
        # Lo·∫°i b·ªè URL
        text = re.sub(r'http[s]?://\S+', ' ', text)
        text = re.sub(r'www\.\S+', ' ', text)
        
        # Lo·∫°i b·ªè email
        text = re.sub(r'\S+@\S+', ' ', text)
        
        # Lo·∫°i b·ªè s·ªë ƒëi·ªán tho·∫°i (ƒë∆°n gi·∫£n)
        text = re.sub(r'\b\d{10,11}\b', ' ', text)
        
        # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát th·ª´a nh∆∞ng gi·ªØ d·∫•u c√¢u c∆° b·∫£n
        text = re.sub(r'[^\w\s\.,!?]', ' ', text)
        
        # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        text = ' '.join(text.split())
        
        return text.strip()
    
    def predict_single(self, text: str) -> Dict:
        """
        Ph√¢n lo·∫°i m·ªôt b√¨nh lu·∫≠n ƒë∆°n l·∫ª
        
        Args:
            text (str): N·ªôi dung b√¨nh lu·∫≠n
            
        Returns:
            Dict: K·∫øt qu·∫£ ph√¢n lo·∫°i v·ªõi label, confidence, description, decision
        """
        if not self.is_loaded:
            return {
                "label": 2,  # Default reject n·∫øu model kh√¥ng load ƒë∆∞·ª£c
                "confidence": 0.0,
                "description": "Model ch∆∞a ƒë∆∞·ª£c load",
                "decision": "reject",
                "reason": "PhoBERT model kh√¥ng kh·∫£ d·ª•ng",
                "error": "PhoBERT model kh√¥ng kh·∫£ d·ª•ng"
            }
        
        if not text or not text.strip():
            return {
                "label": 0,
                "confidence": 0.5,
                "description": "VƒÉn b·∫£n tr·ªëng",
                "processed_text": ""
            }
        
        try:
            # Ti·ªÅn x·ª≠ l√Ω
            processed_text = self.preprocess_text(text)
            
            if not processed_text:
                return {
                    "label": 0,
                    "confidence": 0.5,
                    "description": "VƒÉn b·∫£n tr·ªëng sau x·ª≠ l√Ω",
                    "processed_text": ""
                }
            
            # Tokenize
            inputs = self.tokenizer(
                processed_text,
                return_tensors="pt",
                max_length=256,
                truncation=True,
                padding=True
            )
            
            # Chuy·ªÉn inputs sang device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Predict
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1)
                predicted_label = torch.argmax(logits, dim=-1).item()
                confidence = probabilities[0][predicted_label].item()
            
            # Decision logic theo toxic keywords + PhoBERT
            # Check for common Vietnamese toxic words first
            text_lower = processed_text.lower()
            toxic_keywords = ['ƒëm', 'dm', 'ƒëmm', 'dmm', 'v√£i', 'shit', 'fuck', 
                             'con ch√≥', 'th·∫±ng lol', 'm·∫π ki·∫øp', 'ƒë·ªì ngu', '√≥c ch√≥']
            
            has_toxic_keyword = any(keyword in text_lower for keyword in toxic_keywords)
            
            if has_toxic_keyword:
                # Force reject if contains toxic keywords
                decision = "reject"
                reason = "B√¨nh lu·∫≠n ch·ª©a t·ª´ ng·ªØ kh√¥ng ph√π h·ª£p"
            elif predicted_label == 0 and confidence > 0.7:
                # Only approve clearly positive comments
                decision = "approve"
                reason = f"B√¨nh lu·∫≠n t√≠ch c·ª±c (Label {predicted_label})"
            elif predicted_label in [1, 2]:
                # Reject negative and toxic
                decision = "reject"
                reason = f"B√¨nh lu·∫≠n ti√™u c·ª±c/ƒë·ªôc h·∫°i (Label {predicted_label})"
            else:
                # Low confidence
                decision = "reject"
                reason = f"ƒê·ªô tin c·∫≠y th·∫•p ({confidence:.2f})"
            
            return {
                "label": predicted_label,
                "confidence": float(confidence),
                "description": self.label_descriptions.get(predicted_label, "Unknown"),
                "decision": decision,
                "reason": reason,
                "processed_text": processed_text[:100] + "..." if len(processed_text) > 100 else processed_text,
                "probabilities": {
                    "label_0": float(probabilities[0][0]),
                    "label_1": float(probabilities[0][1]),
                    "label_2": float(probabilities[0][2])
                }
            }
            
        except Exception as e:
            logger.error(f"L·ªói khi predict: {e}")
            return {
                "label": 1,  # Default safe label
                "confidence": 0.0,
                "description": "L·ªói trong qu√° tr√¨nh ph√¢n lo·∫°i",
                "error": str(e)
            }
    
    def predict_batch(self, texts: List[str], batch_size: int = 8) -> List[Dict]:
        """
        Ph√¢n lo·∫°i nhi·ªÅu b√¨nh lu·∫≠n c√πng l√∫c
        
        Args:
            texts (List[str]): Danh s√°ch b√¨nh lu·∫≠n
            batch_size (int): K√≠ch th∆∞·ªõc batch ƒë·ªÉ x·ª≠ l√Ω
            
        Returns:
            List[Dict]: Danh s√°ch k·∫øt qu·∫£ ph√¢n lo·∫°i
        """
        if not self.is_loaded:
            return [{"label": 1, "confidence": 0.0, "error": "Model not loaded"} for _ in texts]
        
        results = []
        
        # X·ª≠ l√Ω theo batch ƒë·ªÉ tr√°nh out of memory
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_results = []
            
            for text in batch_texts:
                result = self.predict_single(text)
                batch_results.append(result)
            
            results.extend(batch_results)
        
        return results
    
    def get_model_info(self) -> Dict:
        """
        L·∫•y th√¥ng tin v·ªÅ model ƒë√£ load
        
        Returns:
            Dict: Th√¥ng tin model
        """
        return {
            "model_path": str(self.model_path),
            "is_loaded": self.is_loaded,
            "device": str(self.device),
            "labels": self.id2label,
            "label_descriptions": self.label_descriptions,
            "model_available": self.model is not None,
            "tokenizer_available": self.tokenizer is not None
        }

# Singleton instance ƒë·ªÉ s·ª≠ d·ª•ng trong to√†n b·ªô ·ª©ng d·ª•ng
phobert_service = PhoBERTService()

def classify_comment(text: str) -> Dict:
    """
    H√†m helper ƒë·ªÉ ph√¢n lo·∫°i m·ªôt b√¨nh lu·∫≠n
    
    Args:
        text (str): N·ªôi dung b√¨nh lu·∫≠n
        
    Returns:
        Dict: K·∫øt qu·∫£ ph√¢n lo·∫°i
    """
    return phobert_service.predict_single(text)

def classify_comments_batch(texts: List[str]) -> List[Dict]:
    """
    H√†m helper ƒë·ªÉ ph√¢n lo·∫°i nhi·ªÅu b√¨nh lu·∫≠n
    
    Args:
        texts (List[str]): Danh s√°ch b√¨nh lu·∫≠n
        
    Returns:
        List[Dict]: Danh s√°ch k·∫øt qu·∫£ ph√¢n lo·∫°i
    """
    return phobert_service.predict_batch(texts) 
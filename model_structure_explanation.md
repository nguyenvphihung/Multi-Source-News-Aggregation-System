# Giáº£i ThÃ­ch Cáº¥u TrÃºc PhoBERT Model

## ğŸ“ Folder: `phobert_toxic_comment_model/`

ÄÃ¢y lÃ  má»™t **PhoBERT model Ä‘Ã£ Ä‘Æ°á»£c fine-tune** Ä‘á»ƒ phÃ¢n loáº¡i bÃ¬nh luáº­n Ä‘á»™c háº¡i tiáº¿ng Viá»‡t.

## ğŸ“„ PhÃ¢n TÃ­ch Tá»«ng File:

### ğŸ§  **Core Model Files:**

#### 1. `model.safetensors` (515MB)
- **Chá»©c nÄƒng**: Chá»©a trá»ng sá»‘ (weights) cá»§a neural network
- **Äá»‹nh dáº¡ng**: SafeTensors (an toÃ n hÆ¡n pickle)
- **KÃ­ch thÆ°á»›c**: 515MB â†’ Model lá»›n, cháº¥t lÆ°á»£ng cao
- **Vai trÃ²**: TrÃ¡i tim cá»§a model, chá»©a kiáº¿n thá»©c Ä‘Ã£ há»c

#### 2. `config.json` (881B)
- **Chá»©c nÄƒng**: Cáº¥u hÃ¬nh kiáº¿n trÃºc model
- **Ná»™i dung**: 
  - Sá»‘ layers, attention heads
  - Hidden size, vocab size
  - Label mapping (0,1,2)
- **Vai trÃ²**: "Báº£n thiáº¿t káº¿" cá»§a model

### ğŸ”¤ **Tokenization Files:**

#### 3. `vocab.txt` (874KB, 63,997 lines)
- **Chá»©c nÄƒng**: Tá»« Ä‘iá»ƒn cá»§a model
- **Ná»™i dung**: 63,997 tá»«/token tiáº¿ng Viá»‡t
- **Vai trÃ²**: Chuyá»ƒn Ä‘á»•i text â†’ numbers mÃ  model hiá»ƒu Ä‘Æ°á»£c

#### 4. `bpe.codes` (1.1MB)
- **Chá»©c nÄƒng**: Byte Pair Encoding rules
- **Vai trÃ²**: Chia nhá» tá»« thÃ nh sub-words (xá»­ lÃ½ tá»« má»›i)
- **VÃ­ dá»¥**: "xin chÃ o" â†’ ["x", "in", "ch", "Ã o"]

#### 5. `tokenizer_config.json` (1.2KB)
- **Chá»©c nÄƒng**: Cáº¥u hÃ¬nh tokenizer
- **Ná»™i dung**: CÃ¡c token Ä‘áº·c biá»‡t (`<s>`, `</s>`, `<pad>`, `<mask>`)
- **Vai trÃ²**: HÆ°á»›ng dáº«n cÃ¡ch xá»­ lÃ½ text input

#### 6. `special_tokens_map.json` (167B)
- **Chá»©c nÄƒng**: Mapping token Ä‘áº·c biá»‡t
- **Ná»™i dung**: 
  - `<s>`: Báº¯t Ä‘áº§u cÃ¢u
  - `</s>`: Káº¿t thÃºc cÃ¢u  
  - `<pad>`: Padding token
  - `<mask>`: Masked token

#### 7. `added_tokens.json` (22B)
- **Chá»©c nÄƒng**: Token Ä‘Æ°á»£c thÃªm vÃ o sau khi train
- **KÃ­ch thÆ°á»›c nhá»**: Chá»‰ 4 dÃ²ng â†’ Ã­t token má»›i

### âš™ï¸ **Training Files:**

#### 8. `training_args.bin` (5.2KB)
- **Chá»©c nÄƒng**: LÆ°u tham sá»‘ training
- **Ná»™i dung**: Learning rate, batch size, epochs, v.v.
- **Vai trÃ²**: ThÃ´ng tin vá» cÃ¡ch model Ä‘Æ°á»£c train

## ğŸ—ï¸ **Kiáº¿n TrÃºc Model:**

```
Input Text (Tiáº¿ng Viá»‡t)
    â†“
Tokenizer (vocab.txt + bpe.codes)
    â†“
Token IDs
    â†“
PhoBERT Layers (model.safetensors)
    â†“
Classification Head
    â†“
Output: Label 0/1/2 + Confidence
```

## ğŸ¯ **Má»¥c ÄÃ­ch Sá»­ Dá»¥ng:**

### âœ… **Hoáº¡t Ä‘á»™ng tá»± Ä‘á»™ng:**
1. **Input**: "dm tháº±ng nÃ o viáº¿t bÃ i nÃ y"
2. **Tokenization**: Chuyá»ƒn thÃ nh token IDs
3. **Model prediction**: PhoBERT xá»­ lÃ½
4. **Output**: Label 2 (Ä‘á»™c háº¡i) vá»›i confidence 0.95

### ğŸ“Š **Label Meanings:**
- **Label 0**: "BÃ i viáº¿t hay quÃ¡!" â†’ An toÃ n
- **Label 1**: "KhÃ´ng Ä‘á»“ng Ã½" â†’ TiÃªu cá»±c nháº¹  
- **Label 2**: "dm, ngu vl" â†’ Äá»™c háº¡i

## ğŸ’¡ **So SÃ¡nh KÃ­ch ThÆ°á»›c:**

| File | KÃ­ch thÆ°á»›c | % Total | Vai trÃ² |
|------|------------|---------|---------|
| `model.safetensors` | 515MB | ~97% | Model weights |
| `vocab.txt` | 874KB | ~1.6% | Vocabulary |
| `bpe.codes` | 1.1MB | ~2% | Tokenization |
| CÃ²n láº¡i | <10KB | <0.1% | Config |

## ğŸš€ **Æ¯u Ä‘iá»ƒm cá»§a PhoBERT:**

1. **Tiáº¿ng Viá»‡t native**: Hiá»ƒu grammar, context Viá»‡t Nam
2. **Pre-trained**: ÄÃ£ há»c tá»« massive Vietnamese corpus  
3. **Fine-tuned**: ÄÆ°á»£c train Ä‘áº·c biá»‡t cho toxic detection
4. **BERT architecture**: State-of-the-art NLP model

## âš¡ **Performance:**

- **Model size**: ~530MB total
- **Vocabulary**: 63,997 tokens (ráº¥t lá»›n)
- **Architecture**: RoBERTa-based (improved BERT)
- **Language**: Vietnamese specialized
- **Task**: Sequence classification (3 classes)

## ğŸ”§ **YÃªu Cáº§u Há»‡ Thá»‘ng:**

- **RAM**: Tá»‘i thiá»ƒu 2GB cho model inference
- **Storage**: 530MB disk space
- **Dependencies**: PyTorch + Transformers
- **CPU/GPU**: CPU Ä‘á»§, GPU tá»‘t hÆ¡n (náº¿u cÃ³)

---

**ğŸ‰ ÄÃ¢y lÃ  má»™t model AI cháº¥t lÆ°á»£ng cao, Ä‘Æ°á»£c train chuyÃªn biá»‡t cho viá»‡c kiá»ƒm duyá»‡t bÃ¬nh luáº­n tiáº¿ng Viá»‡t!** 